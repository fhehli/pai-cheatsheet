\section{Markov Decision Processes}
% {Control based on prob. model}
A MDP is defined by
States $X = \{1,..,n\}$,
Actions $A = \{1,..,m\}$,
Transition probabilities $P(x' \mid x,a)$,
Reward function $r(x,a)$.

\rsubsection*{Policy} $\pi: X \rightarrow A$ or $\pi(a\mid x)$
% {induces a MC} with transition probabilities $P(X_{t+1} = x' \mid  X_t = x) = P(x' \mid  x, \pi(x))$ (det.) or $\sum_a \pi(a \mid  x) P(x' \mid  x, a)$ (rand.)

\rsubsection*{Action-Value Function}
$Q^\pi(x,a)=r(x,a)+\gamma\sum_{x'}P(x'\mid x,a)V^\pi(x')$
\rsubsection*{Value function} 
{$V^\pi(x)$} $ = J(\pi \mid X_0 = x) =Q^\pi(x,\pi(x))\\
\hspace*{3mm}=\mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r(X_t, \pi(X_t)) \mid X_0 = x]\\
\hspace*{1mm}\stackrel{\pi det.}=r(x, \pi(x)) +\gamma \sum_{x'} P(x' \mid x, \pi(x)) V^\pi(x')\\
\hspace*{0mm}\stackrel{\pi rand.}=\sum_a\pi(a|x)\big[r(x, a) +\gamma \sum_{x'} 
P(x' \mid x, a) V^\pi(x')\big]$\\
$\Leftrightarrow V^\pi = (I - \gamma T^\pi)^{-1} r^\pi$
 with $V_i^\pi = V^\pi(i)$,\\
 \hspace*{3mm} $r_i^\pi = r^\pi(i, \pi(i))$ and
$T_{i,j}^\pi = P(j \mid  i,\pi(i))$.

% \vspace*{-1mm}
% \mbox{$V^\pi(x) = \sum_{x'} P(x' \mid  x, \pi(x)) [r(x,\pi(x),x') + \gamma V^\pi(x')]$}

% \mhl{$V^\pi(x) = Q^\pi(x, \pi(x))$} (deterministic policy $\pi$)

% \mhl{$V^\pi(x) = \mathbb{E}_{a' \sim \pi(x)} Q^\pi(x,a')$} (prob. policy $\pi(x)$)

\rsubsection*{Fixed Point Iteration}
Init $V_0^\pi$.
For $t=1,..$ do:\\
$V_t^\pi = r^\pi + \gamma T^\pi V_{t-1}^\pi$ (contraction) 

\rsubsection*{Greedy Policy w.r.t. $V$}
$V$ induces greedy policy
{$\pi_V(x) = \arg\max_a r(x,a) + \gamma \sum_{x'} P(x' \mid  x,a) V(x')$}

% Optimal policy: $\pi^* = \arg\max_a Q^*(x,a)$

Thm: (Bellman) Optimal policy is greedy wrt. its own value function.

% {$V^*(x) = \max_{a}  r(x,a) + \gamma \sum_{x' \in X} P(x' \mid  x,a) V^*(x')$}

% {$ = \max_{a} \mathbb{E}_{x'}[r(x,a) + \gamma V^*(x')] = \max_{a} Q^*(x,a)$}

\rsubsection*{Policy Iteration}
Init arbitrary policy $\pi$.
Until converged: {Compute $V^{\pi}(x)$; compute}
{greedy policy $\pi_G$ w.r.t. $V^{\pi}$; set $\pi \leftarrow \pi_G$}.

% Stop if $V^{\pi_t}(x) = V^{\pi_{t+1}}(x)$.  
PI monotonically improves all values $V^{\pi_{t+1}}(x) \geq V^{\pi_{t}}(x) \forall x$. 
Converges to exact solution in $\mathcal{O}(n^2 m / (1-\gamma))$ iterations.

% \textbf{Q:} \mhl{$Q_t(x,a) = r(x,a) + \gamma \sum_{x'} P(x' \mid  x,a) V_{\textcolor{red}{t-1}}(x')$}

\rsubsection*{Value Iteration}
Init $V_0(x) = \max_a r(x,a)$.
For $t = 1,..$: \\
Set \hbox{$Q_t(x,a)=r(x,a)+\gamma\sum_{x'}P(x'\mid x,a)V_{t-1}(x')$} and
{$V_t(x) = \max_a Q_t(x,a)$}.
Stop if $||V_t - V_{t-1}||_\infty \leq \epsilon$, then choose
greedy policy w.r.t. $V_t$. Converges to $\epsilon$-optimal 
policy in polynomially many iterations.

\section{POMDP}
Obtain only noisy observations $Y_t$ of state $X_t$.
% Finite horizon $T$: exp. \#belief states. BUT: most belief
% states never reached $\rightarrow$ discretize space by sampling.
Solve by modelling $P(X_t\mid y_{1:t})$.
% Use policy gradients with parametric policy.

\rsubsection*{Belief States} POMDP as MDP where states $\equiv$ beliefs
$P(X_t \mid  y_{1:t})$ in the orig. POMDP. 
%  States $\mathcal{B} = \{b : \{1,..,n\} \rightarrow [0,1], \sum_{x \in X} b(x) = 1\}$,


{Actions $\mathcal{A} = \{1,..,m\}$},
 Transitions: $P(Y_{t+1} = y \mid  b_t, a_t) = \sum_{x,x'} b_t(x) P(x' \mid  x, a_t) P(y \mid  x')$;
$b_{t+1}(x') = \frac{1}{Z} \sum_x b_t(x) P(X_{t+1} = x' \mid  X_t = x, a_t) P(y_{t+1} \mid  x')$

Reward: $r(b_t, a_t) = \sum_x b_t(x) r(x, a_t)$

